\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm, booktabs}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}

\title{Summary of Key Probability Distributions and Statistical Formulas}
\author{}
\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{General Probability and Statistics Formulas}

\subsection{Expected Value and Variance}
\begin{align*}
    E[X] &= \sum_{x} x P(X = x) &\text{(Discrete)} \\
    E[X] &= \int_{-\infty}^{\infty} x f(x) dx &\text{(Continuous)}
\end{align*}
\begin{align*}
    \operatorname{Var}(X) &= E[X^2] - (E[X])^2
\end{align*}

\subsection{Covariance and Linearity Properties}
\begin{align*}
    \operatorname{Cov}(X, Y) &= E[XY] - E[X]E[Y] \\
    \operatorname{Var}(X + Y) &= \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X,Y) \\
    \operatorname{Cov}(aX + b, cY + d) &= ac \operatorname{Cov}(X, Y) \\
    \operatorname{Cov}(X, X) &= \operatorname{Var}(X)
\end{align*}

\subsection{Correlation Coefficient}
The correlation coefficient measures the strength and direction of the linear relationship between two variables:
\begin{align*}
    \rho_{X,Y} &= \frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y},
\end{align*}
where $\sigma_X = \sqrt{\operatorname{Var}(X)}$ and $\sigma_Y = \sqrt{\operatorname{Var}(Y)}$.

\section{Probability Distributions}

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Distribution & PMF/PDF & $E[X]$ & $\operatorname{Var}(X)$ \\
        \midrule
        Bernoulli $(p)$ & $P(X = x) = p^x(1-p)^{1-x}$ & $p$ & $p(1-p)$ \\
        Binomial $(n, p)$ & $\binom{n}{k} p^k (1-p)^{n-k}$ & $np$ & $np(1-p)$ \\
        Geometric $(p)$ & $(1-p)^{k-1} p$ & $\frac{1}{p}$ & $\frac{1-p}{p^2}$ \\
        Poisson $(\lambda)$ & $\frac{\lambda^k e^{-\lambda}}{k!}$ & $\lambda$ & $\lambda$ \\
        Exponential $(\lambda)$ & $\lambda e^{-\lambda x}, x \geq 0$ & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$ \\
        Normal $(\mu, \sigma^2)$ & $\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ & $\mu$ & $\sigma^2$ \\
        Uniform $(a, b)$ & $\frac{1}{b-a}, a \leq x \leq b$ & $\frac{a+b}{2}$ & $\frac{(b-a)^2}{12}$ \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Probability Distributions}
    \label{tab:distributions}
\end{table}

\section{Likelihood, MLE, and Fisher Information}

\subsection{Likelihood Function}
For a set of independent and identically distributed (i.i.d.) observations $X_1, X_2, \dots, X_n$ from a probability distribution with parameter $\theta$, the likelihood function is:
\begin{align*}
    L(\theta \mid X_1, \dots, X_n) = \prod_{i=1}^{n} f(X_i \mid \theta).
\end{align*}

\subsection{Maximum Likelihood Estimation (MLE)}
To find the MLE, we maximize the likelihood function (or its log-likelihood $\ell(\theta) = \log L(\theta)$) by solving:
\begin{align*}
    \frac{d\ell(\theta)}{d\theta} = 0.
\end{align*}

\subsection{Fisher Information}
The Fisher Information measures the amount of information a random variable carries about a parameter:
\begin{align*}
    I(\theta) = -E\left[ \frac{d^2 \ell(\theta)}{d\theta^2} \right].
\end{align*}
The CramÃ©r-Rao Lower Bound states that the variance of any unbiased estimator $\hat{\theta}$ satisfies:
\begin{align*}
    \operatorname{Var}(\hat{\theta}) \geq \frac{1}{I(\theta)}.
\end{align*}

\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm, booktabs}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}

\title{Summary of Key Probability Distributions and Statistical Formulas}
\author{}
\date{\today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{General Probability and Statistics Formulas}

\subsection{Conditional Probability}
The probability of an event $A$ given another event $B$ has occurred is given by:
\begin{align*}
    P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad \text{if } P(B) > 0.
\end{align*}

P(A \cap B) = P(A) \times P(B|A)


\subsection{Bayes' Theorem}
Bayes' theorem relates conditional probabilities:
\begin{align*}
    P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}.
\end{align*}

\subsection{Law of Total Probability}
If $B_1, B_2, ..., B_n$ form a partition of the sample space, then for any event $A$:
\begin{align*}
    P(A) = \sum_{i=1}^{n} P(A \mid B_i) P(B_i).
\end{align*}

\subsection{Joint Probability Distribution}
For two discrete random variables $X$ and $Y$, the joint probability mass function is given by:
\begin{align*}
    P(X = x, Y = y) = P(X = x \mid Y = y) P(Y = y).
\end{align*}

\subsection{Law of Large Numbers}
The Law of Large Numbers states that as the number of trials $n$ increases, the sample mean $\bar{X}$ converges to the true mean $E[X]$:
\begin{align*}
    \lim_{n \to \infty} \bar{X} = E[X].
\end{align*}

\subsection{Marginal Distribution}
For joint distribution $P(X, Y)$, the marginal distributions are obtained by summing/integrating out the other variable:
\begin{align*}
    P(X = x) = \sum_{y} P(X = x, Y = y), \quad f_X(x) = \int f_{X,Y}(x, y) dy.
\end{align*}

\subsection{Sum of Random Variables (Convolution)}
For independent random variables $X$ and $Y$, the probability distribution of their sum $Z = X + Y$ is given by the convolution:
\begin{align*}
    f_Z(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z - x) dx.
\end{align*}

\subsection{Central Limit Theorem}
For a sequence of i.i.d. random variables $X_1, X_2, ..., X_n$ with mean $\mu$ and variance $\sigma^2$, the sum (or average) converges in distribution to a normal distribution:
\begin{align*}
    \frac{\sum_{i=1}^{n} X_i - n\mu}{\sigma \sqrt{n}} \to N(0,1) \quad \text{as } n \to \infty.
\end{align*}

\section{Conclusion}
This document summarizes key probability distributions, expected value and variance properties, covariance and correlation, and fundamental statistical estimation techniques. These principles are crucial in probability theory and data analysis.

\end{document}


\end{document}
